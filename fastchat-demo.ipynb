{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c92bd3d-4cb4-4d3c-8b61-a35e7680b439",
   "metadata": {},
   "source": [
    "# FastChat Demo\n",
    "This notebook uses the [OpenAI REST API](https://platform.openai.com/docs/api-reference/introduction) to interact with LLMs hosted in a [FastChat](https://github.com/lm-sys/FastChat) deployment.\n",
    "FastChat only supports chat completion and embeddings API endpoints.\n",
    "For use on [jupyterhub.sdsu.edu](jupyterhub.sdsu.edu) select the image \"Stack PRP\". The Stack PRP image and FastChat both use Open AI API v0.28.1.\n",
    "\n",
    "The OpenAI REST API endpoint is availbale at [https://sdsu-rci-fastchat.nrp-nautilus.io/v1](https://sdsu-rci-fastchat.nrp-nautilus.io/v1).\n",
    "\n",
    "Your credentials should be stored in a file `env.yaml`. The API key will be shared with you via your instructor. Your `env.yaml` file should mimic the structure of the provided sample `env-template.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc422fa-e077-433c-9f05-9a2fe4868eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b71a0-cc96-4ac1-92b7-c657eb136369",
   "metadata": {},
   "source": [
    "## Import Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e175a3-b648-4774-8a7b-85eb7ff90cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sdsu-rci-fastchat-test.nrp-nautilus.io/v1\n"
     ]
    }
   ],
   "source": [
    "with open('../env.yaml', 'r') as f:\n",
    "    env = yaml.safe_load(f)\n",
    "\n",
    "print(env[\"fastchat\"][\"base_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cab1a9-14f3-4407-8694-eb12438f9276",
   "metadata": {},
   "source": [
    "## Setup API Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c120c37-7460-4f45-884a-bb6d983b7c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"THUDM/CogVLM\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1711214415,\n",
      "      \"owned_by\": \"fastchat\",\n",
      "      \"root\": \"THUDM/CogVLM\",\n",
      "      \"parent\": null,\n",
      "      \"permission\": [\n",
      "        {\n",
      "          \"id\": \"modelperm-3VgQB7H2hXn5stePKtGZg4\",\n",
      "          \"object\": \"model_permission\",\n",
      "          \"created\": 1711214415,\n",
      "          \"allow_create_engine\": false,\n",
      "          \"allow_sampling\": true,\n",
      "          \"allow_logprobs\": true,\n",
      "          \"allow_search_indices\": true,\n",
      "          \"allow_view\": true,\n",
      "          \"allow_fine_tuning\": false,\n",
      "          \"organization\": \"*\",\n",
      "          \"group\": null,\n",
      "          \"is_blocking\": false\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1711214415,\n",
      "      \"owned_by\": \"fastchat\",\n",
      "      \"root\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
      "      \"parent\": null,\n",
      "      \"permission\": [\n",
      "        {\n",
      "          \"id\": \"modelperm-RbDZPwxX2vW9oPNb3NJtTq\",\n",
      "          \"object\": \"model_permission\",\n",
      "          \"created\": 1711214415,\n",
      "          \"allow_create_engine\": false,\n",
      "          \"allow_sampling\": true,\n",
      "          \"allow_logprobs\": true,\n",
      "          \"allow_search_indices\": true,\n",
      "          \"allow_view\": true,\n",
      "          \"allow_fine_tuning\": false,\n",
      "          \"organization\": \"*\",\n",
      "          \"group\": null,\n",
      "          \"is_blocking\": false\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = env[\"fastchat\"][\"api_key\"]\n",
    "openai.api_base = env[\"fastchat\"][\"base_url\"]\n",
    "\n",
    "# Test config by printing available models\n",
    "models = openai.Model.list()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923c114-8548-47d1-8735-792855695ede",
   "metadata": {},
   "source": [
    "## Preprocess paper.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1089429-5caa-47d9-ada0-e25b49a1de5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paper.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_filename = env['file_name']\n",
    "text_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2730a7c3-759f-4ac8-9649-5bf3b07c6e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw transcript character count: 5129\n",
      "Raw transcript word count: 696\n",
      "Raw transcript line count: 7\n"
     ]
    }
   ],
   "source": [
    "transcript_raw = \"\"\n",
    "\n",
    "with open(text_filename, 'r') as f:\n",
    "    transcript_raw = f.read()\n",
    "\n",
    "# Calculate and print info about raw file\n",
    "rawCharCount = len(transcript_raw)\n",
    "rawWordCount = len(transcript_raw.split())\n",
    "rawLineCount = len(transcript_raw.split(\"\\n\"))\n",
    "\n",
    "print(f\"Raw transcript character count: {rawCharCount}\")\n",
    "print(f\"Raw transcript word count: {rawWordCount}\")\n",
    "print(f\"Raw transcript line count: {rawLineCount}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca5b14d-532e-4cd3-bea2-aee8d64f5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process transcript as a list to make it iterable\n",
    "transcript_transform = transcript_raw.split(\"\\n\")\n",
    "transcript_concat = \"\".join(transcript_transform)\n",
    "\n",
    "# some models have a word limit in which you can change here, though not limiting this word count might be okay as some tokenizers can handle extra words\n",
    "final_sentence = transcript_concat[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24194b51-8c88-4c52-ae2c-3b411ce524c0",
   "metadata": {},
   "source": [
    "## Ask the LLM to Perform the Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "266411d7-f509-44cb-8b41-2c6f6b86ebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The researcher is trying to accomplish the following three items:\n",
      "\n",
      "1. Introduce a new attention mechanism called the Gaussian Adaptive Attention Mechanism (GAAM) that improves the standard self-attention mechanism in Transformers.\n",
      "2. Enhance the Transformer model's Attention mechanism to improve its ability to interpret and process sequential and spatial data.\n",
      "3. Improve the performance of the Transformer model across various domains such as multimedia recommendation, image classification, and text classification by using GAAM.\n",
      "4. Provide a more interpretable framework for Artificial Intelligence (AI) by introducing GAAM, which offers improved accuracy, robustness, and user experience across diverse real-world applications.\n"
     ]
    }
   ],
   "source": [
    "# Model can be replaced with the model id from the previous call\n",
    "# \"vicuna-33b-v1.3\" is the second model\n",
    "model = models.data[1].id\n",
    "\n",
    "initial_prompt = \"You will be given the introduction to a scientific paper in Artificial Intelligence. \\\n",
    "From this introduction: Provide the top 3 items discussed and what the researcher is trying to accomplish.\"\n",
    "\n",
    "prompt = final_sentence\n",
    "\n",
    "# create a chat completion\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": initial_prompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "# print the completion\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65adb2ff-3eea-4436-b711-a2000d38fae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      " researcher\n",
      " is trying\n",
      " to accomplish\n",
      " the following\n",
      ":\n",
      "\n",
      "\n",
      "1\n",
      ". Int\n",
      "roduce\n",
      " a significant\n",
      " enhancement\n",
      " to the\n",
      " Transformer\n",
      " model'\n",
      "s Att\n",
      "ention mechanism\n",
      " by introdu\n",
      "cing the\n",
      " (Multi\n",
      "-Head\n",
      ") Gaussian\n",
      " Adapt\n",
      "ive Att\n",
      "ention Mechan\n",
      "ism (\n",
      "GAAM\n",
      ").\n",
      "\n",
      "2.\n",
      " Impro\n",
      "ve the\n",
      " Transformer\n",
      " model'\n",
      "s cap\n",
      "ability to\n",
      " interpret and\n",
      " process sequ\n",
      "ential and\n",
      " spatial data\n",
      ".\n",
      "\n",
      "3.\n",
      " Impro\n",
      "ve model\n",
      " performance in\n",
      " various domains\n",
      " such as\n",
      " multimedia recommendation\n",
      ", image\n",
      " classification,\n",
      " and text\n",
      " classification.\n",
      "\n",
      "4\n",
      ". Prov\n",
      "ide a\n",
      " more interpre\n",
      "table framework\n",
      " for Art\n",
      "ificial\n",
      " Intelligence\n",
      " by address\n",
      "ing the\n",
      " critical need\n",
      " for trans\n",
      "parency\n",
      " and trust\n",
      "worthiness\n",
      " in real\n",
      "-world\n",
      " AI\n",
      " systems.\n",
      "\n",
      "5\n",
      ". Create\n",
      " a mechanism\n",
      " that lear\n",
      "ns both\n",
      " the mean\n",
      " and variance\n",
      " of input\n",
      " features in\n",
      " a Multi\n",
      "-Head\n",
      "ed setting\n",
      ", allowing\n",
      " for a\n",
      " focused approach\n",
      " to different\n",
      " skewn\n",
      "ess aspects\n",
      " in data\n",
      " subsets.\n",
      "\n",
      "6\n",
      ". Gener\n",
      "ate local\n",
      " attention outputs\n",
      " from each\n",
      " head and\n",
      " combine them\n",
      " to construct\n",
      " a compreh\n",
      "ensive Global\n",
      " Attention\n",
      " map.\n",
      "\n",
      "7\n",
      ". Allow\n",
      " each head\n",
      " independently adjust\n",
      " its mean\n",
      " and variance\n",
      ", allowing\n",
      " for a\n",
      " more robust\n",
      " and flexible\n",
      " approach to\n",
      " different data\n",
      " characteristics,\n",
      " including asym\n",
      "metries\n",
      " and non\n",
      "-G\n",
      "aussian tra\n",
      "its.\n"
     ]
    }
   ],
   "source": [
    "#How to use the stream parameter\n",
    "model = models.data[1].id\n",
    "\n",
    "initial_prompt = \"You will be given the introduction to a scientific paper in Artificial Intelligence. \\\n",
    "From this introduction: Provide the top 3 items discussed and what the researcher is trying to accomplish.\"\n",
    "\n",
    "prompt = final_sentence\n",
    "\n",
    "# create a chat completion\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": initial_prompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "  ],\n",
    " stream=True\n",
    ")\n",
    "\n",
    "# print the completion\n",
    "for chunk in completion:\n",
    "    try:\n",
    "        print(chunk.choices[0].delta.content)\n",
    "    except (KeyError,AttributeError):\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19c29c59-326b-4b89-87c2-267079dd0526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top three items discussed in the introduction are:\n",
      "\n",
      "\n",
      "1.\n",
      " Attention mechanisms have significantly advanced the field of sequence modeling,\n",
      " particularly in NLP and various branches of signal processing.\n",
      "\n",
      "2.\n",
      " Attention mechanisms in Transformer models have inherent limitations in handling long-range dependencies due to quadratic complexity.\n",
      "\n",
      "3.\n",
      " Ongoing research continues to address these challenges and seeks more efficient ways to model long sequences and capture global context dependencies.\n",
      "\n",
      "\n",
      "The researcher is trying to accomplish the following:\n",
      "\n",
      "\n",
      "1.\n",
      " Introduce a significant enhancement to the Transformer model's Attention mechanism:\n",
      " the (Multi-Head) Gaussian Adaptive Attention Mechanism (GAAM).\n",
      "\n",
      "2.\n",
      " Improve the capability of the Transformer model to interpret and process sequential and spatial data using a more context-sensitive and interpretable approach.\n",
      "\n",
      "3.\n",
      " Enhance the model performance in various domains,\n",
      " including multimedia recommendation,\n",
      " image classification,\n",
      " and text classification.\n"
     ]
    }
   ],
   "source": [
    "#How to use the stream parameter to collect chuks of full sentences\n",
    "model = models.data[1].id\n",
    "\n",
    "initial_prompt = \"You will be given the introduction to a scientific paper in Artificial Intelligence. \\\n",
    "From this introduction: Provide the top 3 items discussed and what the researcher is trying to accomplish.\"\n",
    "\n",
    "prompt = final_sentence\n",
    "\n",
    "# create a chat completion\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": initial_prompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "  ],\n",
    " stream=True\n",
    ")\n",
    "\n",
    "punctuation_marks = [\",\",\"?\",\"!\",\".\",\":\",\"*\"]\n",
    "sentence = \"\"\n",
    "# print the completion\n",
    "for chunk in completion:\n",
    "    try:\n",
    "        for char in chunk.choices[0].delta.content:\n",
    "            if char in punctuation_marks:\n",
    "                sentence += char \n",
    "                print(sentence)\n",
    "                sentence = \"\"\n",
    "            else:\n",
    "                sentence += char\n",
    "            \n",
    "    except (KeyError,AttributeError):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6297d7d-b6a1-40e2-ad31-5223124827f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831afcd-1990-4fe5-9148-19b43fd25f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastchat",
   "language": "python",
   "name": "fastchat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
